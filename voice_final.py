# -*- coding: utf-8 -*-
"""Final_voice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YHbpnJj3fLIRF5zUYr6js2gHz-r3-IRp
"""


from google.colab import files
files.upload()  
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!pip install -q kaggle
!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio
!unzip -q ravdess-emotional-speech-audio.zip -d ravdess_data
print("‚úÖ RAVDESS dataset extracted to: ravdess_data")
!pip install -q kagglehub
!pip install tensorflow
import kagglehub
import os

iemocap_path = kagglehub.dataset_download("decoder05/iemocap")  
print("\n‚úÖ IEMOCAP dataset downloaded.")
print("üìÇ Path to dataset:", iemocap_path)
for root, dirs, files in os.walk(iemocap_path):
    print(f"üìÅ {root}")
    for d in dirs:
        print(f"   ‚îî‚îÄ‚îÄ üìÇ {d}")
    for f in files[:5]:
        print(f"   ‚îî‚îÄ‚îÄ üìÑ {f}")
    break  
!pip install tensorflow
import os
import numpy as np
import librosa
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Activation,
                                     Dropout, Reshape, Bidirectional, GRU, Dense, Layer)
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)
    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1),
                                 initializer="normal", trainable=True)
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),
                                 initializer="zeros", trainable=True)
        super().build(input_shape)
    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        return tf.keras.backend.sum(x * a, axis=1)
def extract_features(file_path, sr=22050, n_mfcc=13):
    y, sr = librosa.load(file_path, sr=sr)
    y, _ = librosa.effects.trim(y)

    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)
    rms = librosa.feature.rms(y=y)
    pitches, _ = librosa.piptrack(y=y, sr=sr)
    pitch = np.mean(pitches, axis=0, keepdims=True)

    min_len = min(mfcc.shape[1], chroma.shape[1], contrast.shape[1],
                  tonnetz.shape[1], rms.shape[1], pitch.shape[1])

    features = np.vstack([
        mfcc[:, :min_len],
        chroma[:, :min_len],
        contrast[:, :min_len],
        tonnetz[:, :min_len],
        rms[:, :min_len],
        pitch[:, :min_len]
    ])

    return features.T
def find_iemocap_root(base_path):
    for root, dirs, _ in os.walk(base_path):
        if all(f"Session{i}" in dirs for i in range(1, 6)):
            return root
    raise FileNotFoundError("‚ùå Sessions not found in path. Check dataset structure.")

def load_iemocap(base_path):
    base_path = find_iemocap_root(base_path)
    label_map = {
        'neu': 0, 'hap': 0, 'exc': 0,        
        'sad': 1, 'fru': 1,                  
        'ang': 2, 'fea': 2, 'dis': 2         
    }

    X, y = [], []

    for i in range(1, 6):
        eval_dir = os.path.join(base_path, f"Session{i}", "dialog", "EmoEvaluation")
        wav_dir = os.path.join(base_path, f"Session{i}", "sentences", "wav")

        for file in os.listdir(eval_dir):
            if not file.endswith(".txt"): continue
            with open(os.path.join(eval_dir, file)) as f:
                for line in f:
                    if not line.startswith("["): continue
                    parts = line.strip().split("\t")
                    if len(parts) < 3: continue
                    utt_id, emo = parts[1], parts[2]
                    if emo not in label_map: continue

                    speaker = "_".join(utt_id.split("_")[:-1])
                    wav_path = os.path.join(wav_dir, speaker, utt_id + ".wav")

                    if os.path.exists(wav_path):
                        features = extract_features(wav_path)
                        if features is not None:
                            X.append(features)
                            y.append(label_map[emo])

    print(f"‚úÖ Loaded {len(X)} samples from IEMOCAP")
    return np.array(X, dtype=object), np.array(y)
def preprocess(X, y, max_len=400):
    X_pad = pad_sequences(X, maxlen=max_len, dtype='float32', padding='post', truncating='post')
    X_pad = np.expand_dims(X_pad, -1)
    y_cat = to_categorical(y, num_classes=3)
    return train_test_split(X_pad, y_cat, test_size=0.2, stratify=y, random_state=42), y

def build_model(input_shape):
    inp = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), padding='same')(inp)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)

    x = Reshape((input_shape[0], -1))(x)
    x = Bidirectional(GRU(64, return_sequences=True))(x)
    x = Bidirectional(GRU(64, return_sequences=True))(x)
    x = Dropout(0.3)(x)
    x = Attention()(x)

    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    out = Dense(3, activation='softmax')(x)

    return Model(inp, out)


import kagglehub
iemocap_dir = kagglehub.dataset_download("decoder05/iemocap")
print("üìÅ Downloaded IEMOCAP dataset at:", iemocap_dir)

(X_train, X_val, y_train, y_val), y_raw = preprocess(*load_iemocap(iemocap_dir))

y_int = np.argmax(y_train, axis=1)
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_int), y=y_int)
class_weights = dict(enumerate(class_weights))

model = build_model(X_train.shape[1:])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-6),
    ModelCheckpoint("pretrained_iemocap_model.h5", save_best_only=True)
]


history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                    epochs=30, batch_size=32,
                    callbacks=callbacks,
                    class_weight=class_weights)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy"); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss"); plt.legend()
plt.show()

preds = model.predict(X_val)
y_true = np.argmax(y_val, axis=1)
y_pred = np.argmax(preds, axis=1)

print("\nüìà Classification Report:")
print(classification_report(y_true, y_pred, target_names=["Low", "Medium", "High"]))
print("‚úÖ Balanced Accuracy:", balanced_accuracy_score(y_true, y_pred))

sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()


import os
import json
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from dataclasses import dataclass, asdict
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, roc_curve, auc
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, CSVLogger
from tensorflow.keras import regularizers
import librosa
import shutil
import tensorflow as tf
@dataclass
class Config:
    ravdess_dir: str = "ravdess_data"
    pretrained_model_path: str = "pretrained_iemocap_model.h5"
    output_dir: str = "outputs"
    sample_rate: int = 22050
    max_sequence_length: int = 400
    n_mfcc: int = 13
    n_chroma: int = 12
    n_contrast: int = 7
    n_tonnetz: int = 6
    batch_size: int = 32
    epochs: int = 50
    learning_rate: float = 1e-4
    validation_split: float = 0.2
    test_split: float = 0.1
    early_stopping_patience: int = 7
    reduce_lr_patience: int = 3
    reduce_lr_factor: float = 0.3
    min_lr: float = 1e-6
    emotion_mapping = None
    class_names = None

    def __post_init__(self):
        if self.emotion_mapping is None:
            self.emotion_mapping = {
                'low': ['01', '03', '04'],
                'medium': ['02', '05', '06'],
                'high': ['07', '08']
            }
        if self.class_names is None:
            self.class_names = ['Low', 'Medium', 'High']

    def save_config_json_safe(self, filepath: str):
        with open(filepath, 'w') as f:
            json.dump(asdict(self), f, indent=2)
class Logger:
    @staticmethod
    def setup_logger(log_file: str) -> logging.Logger:
        logger = logging.getLogger('RAVDESSFineTune')
        logger.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh = logging.FileHandler(log_file)
        ch = logging.StreamHandler()
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        logger.addHandler(fh)
        logger.addHandler(ch)
        return logger

class Attention(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1),
                                 initializer="normal", trainable=True)
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),
                                 initializer="zeros", trainable=True)
        super().build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        return tf.keras.backend.sum(x * a, axis=1)

    def get_config(self):
        return super().get_config()
class AudioProcessor:
    def __init__(self, config: Config):
        self.config = config

    def extract_features(self, file_path: str) -> np.ndarray:
        try:
            y, sr = librosa.load(file_path, sr=self.config.sample_rate)
            y, _ = librosa.effects.trim(y, top_db=20)
            if len(y) < sr * 0.5:
                y = np.pad(y, (0, int(sr * 0.5) - len(y)))
            return self._extract_all_features(y, sr)
        except:
            return None

    def _extract_all_features(self, y, sr):
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.config.n_mfcc)
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)
        rms = librosa.feature.rms(y=y)
        zcr = librosa.feature.zero_crossing_rate(y=y)
        min_len = min(mfcc.shape[1], chroma.shape[1], contrast.shape[1], tonnetz.shape[1],
                      rms.shape[1], zcr.shape[1])
        features = np.vstack([
            mfcc[:, :min_len],
            chroma[:, :min_len],
            contrast[:, :min_len],
            tonnetz[:, :min_len],
            rms[:, :min_len],
            zcr[:, :min_len]
        ])
        return features.T
class RAVDESSDataLoader:
    def __init__(self, config: Config):
        self.config = config
        self.processor = AudioProcessor(config)

    def load_data(self):
        X, y = [], []
        emotion_to_label = {code: i for i, (_, codes) in enumerate(self.config.emotion_mapping.items()) for code in codes}
        files = list(Path(self.config.ravdess_dir).rglob("*.wav"))
        for path in tqdm(files, desc="Loading audio"):
            parts = path.name.split("-")
            if len(parts) < 3: continue
            emo = parts[2]
            if emo not in emotion_to_label: continue
            label = emotion_to_label[emo]
            features = self.processor.extract_features(str(path))
            if features is not None:
                X.append(features)
                y.append(label)
        return np.array(X, dtype=object), np.array(y)

    def preprocess_data(self, X, y):
        X_pad = pad_sequences(X, maxlen=self.config.max_sequence_length,
                              dtype='float32', padding='post', truncating='post')
        X_pad = np.expand_dims(X_pad, -1)
        y_cat = to_categorical(y, num_classes=len(self.config.class_names))
        X_temp, X_test, y_temp, y_test = train_test_split(
            X_pad, y_cat, test_size=self.config.test_split, stratify=y, random_state=42)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=self.config.validation_split,
            stratify=np.argmax(y_temp, axis=1), random_state=42)
        return X_train, X_val, X_test, y_train, y_val, y_test

class ModelManager:
    def __init__(self, config: Config):
        self.config = config

    def load_and_adapt_model(self):
        model = load_model(self.config.pretrained_model_path, custom_objects={'Attention': Attention})
        if model.output_shape[-1] != len(self.config.class_names):
            x = model.layers[-2].output
            x = Dropout(0.5)(x)
            out = Dense(len(self.config.class_names), activation='softmax',
                        kernel_regularizer=regularizers.l2(1e-4))(x)
            model = Model(inputs=model.input, outputs=out)
        return model

    def create_callbacks(self):
        return [
            EarlyStopping(monitor='val_loss', patience=self.config.early_stopping_patience, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', patience=self.config.reduce_lr_patience,
                              factor=self.config.reduce_lr_factor, min_lr=self.config.min_lr),
            ModelCheckpoint(os.path.join(self.config.output_dir, 'best_model.h5'),
                            monitor='val_accuracy', save_best_only=True),
            TensorBoard(log_dir=os.path.join(self.config.output_dir, 'tensorboard')),
            CSVLogger(os.path.join(self.config.output_dir, 'training_log.csv'))
        ]

def save_evaluation_outputs(model, X_test, y_test, config, prefix="final"):
    y_pred_proba = model.predict(X_test)
    y_pred = np.argmax(y_pred_proba, axis=1)
    y_true = np.argmax(y_test, axis=1)

    
    np.save(os.path.join(config.output_dir, f"{prefix}_softmax_outputs.npy"), y_pred_proba)
    np.save(os.path.join(config.output_dir, f"{prefix}_y_test.npy"), y_test)

    
    df = pd.DataFrame(y_pred_proba, columns=["prob_Low", "prob_Medium", "prob_High"])
    df["true_label"] = y_true
    df["predicted_label"] = y_pred
    df.to_csv(os.path.join(config.output_dir, f"{prefix}_softmax_outputs.csv"), index=False)

    
    report = classification_report(y_true, y_pred, target_names=config.class_names, output_dict=True)
    pd.DataFrame(report).transpose().to_csv(os.path.join(config.output_dir, f"{prefix}_classification_report.csv"))

    
    bal_acc = balanced_accuracy_score(y_true, y_pred)
    pd.DataFrame([{"Balanced Accuracy": bal_acc}]).to_csv(
        os.path.join(config.output_dir, f"{prefix}_balanced_accuracy.csv"), index=False)

    
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=config.class_names, yticklabels=config.class_names)
    plt.title(f'{prefix.capitalize()} Confusion Matrix')
    plt.savefig(os.path.join(config.output_dir, f"{prefix}_confusion_matrix.png"))
    plt.close()

    
    fpr, tpr, roc_auc = {}, {}, {}
    for i in range(len(config.class_names)):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    plt.figure()
    for i in range(len(config.class_names)):
        plt.plot(fpr[i], tpr[i], label=f"{config.class_names[i]} (AUC = {roc_auc[i]:.2f})")
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f'{prefix.capitalize()} ROC Curve')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.savefig(os.path.join(config.output_dir, f"{prefix}_roc_curve.png"))
    plt.close()
def main():
    config = Config()
    os.makedirs(config.output_dir, exist_ok=True)
    logger = Logger.setup_logger(os.path.join(config.output_dir, 'training.log'))
    config.save_config_json_safe(os.path.join(config.output_dir, 'config.json'))

    try:
        loader = RAVDESSDataLoader(config)
        X, y = loader.load_data()
        X_train, X_val, X_test, y_train, y_val, y_test = loader.preprocess_data(X, y)

        model_manager = ModelManager(config)
        model = model_manager.load_and_adapt_model()
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate),
                      loss='categorical_crossentropy', metrics=['accuracy'])

        callbacks = model_manager.create_callbacks()
        model.fit(X_train, y_train, validation_data=(X_val, y_val),
                  epochs=config.epochs, batch_size=config.batch_size,
                  callbacks=callbacks, verbose=1, shuffle=True)

        model.save(os.path.join(config.output_dir, 'final_model.h5'))
        save_evaluation_outputs(model, X_test, y_test, config, prefix="final")

        shutil.make_archive(config.output_dir, 'zip', config.output_dir)
        print(f"\n‚úÖ Download your results: {config.output_dir}.zip")

    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise

if __name__ == "__main__":
    main()
